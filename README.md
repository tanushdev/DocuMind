# DocuMind ğŸ§ ğŸ“„

> **Production-Grade AI Document Intelligence System**

A sophisticated Retrieval-Augmented Generation (RAG) system demonstrating elite-tier systems and ML engineering. Built with **Python + Go**, featuring custom vector search, distributed architecture, and performance-first design.

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)
[![Go](https://img.shields.io/badge/Go-1.21+-00ADD8.svg)](https://golang.org)
[![Redis](https://img.shields.io/badge/Redis-7+-red.svg)](https://redis.io)
[![Docker](https://img.shields.io/badge/Docker-Compose-2496ED.svg)](https://docker.com)

---

## ğŸ¯ Project Overview

DocuMind allows users to:
- **Upload documents** (PDF / TXT)
- **Ask natural-language questions**
- **Receive AI-powered answers with cited sources**

### Key Differentiators

| Aspect | What We Built | Why It Matters |
|--------|---------------|----------------|
| **Vector Search** | Custom HNSW in Go | Demonstrates algorithmic depth, not just "using FAISS" |
| **RAG Pipeline** | Manual implementation | No LangChain = full understanding and control |
| **Architecture** | True microservices | Go + Python with async boundaries |
| **Performance** | Caching + metrics | Production-ready observability |

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                   CLIENT                                     â”‚
â”‚                           (React / HTML UI)                                  â”‚
â”‚                              Port 3000                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          PYTHON ORCHESTRATOR                                 â”‚
â”‚                           (FastAPI - Port 8000)                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚  Document   â”‚ â”‚  Embedding  â”‚ â”‚ Re-ranking  â”‚ â”‚    Context Assembly     â”‚â”‚
â”‚  â”‚  Chunking   â”‚ â”‚  Generation â”‚ â”‚  (Cross-    â”‚ â”‚    + LLM Integration    â”‚â”‚
â”‚  â”‚  (Manual)   â”‚ â”‚  (SBERT)    â”‚ â”‚  Encoder)   â”‚ â”‚    (Ollama/HF)          â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                      â”‚                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚              Background Task Processing (asyncio)                       â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                                â”‚                        â”‚
          â–¼                                â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GO VECTOR SVC   â”‚          â”‚     REDIS        â”‚       â”‚   LLM SERVICE    â”‚
â”‚  (Port 8001)     â”‚          â”‚  (Port 6379)     â”‚       â”‚   (Ollama)       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚       â”‚   (Port 11434)   â”‚
â”‚  â”‚  HNSW      â”‚  â”‚          â”‚  â”‚  Embedding â”‚  â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  â”‚  Index     â”‚  â”‚          â”‚  â”‚  Cache     â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚          â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚  Brute     â”‚  â”‚          â”‚  â”‚  Query     â”‚  â”‚
â”‚  â”‚  Force     â”‚  â”‚          â”‚  â”‚  Cache     â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚          â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚  Cosine    â”‚  â”‚          â”‚  â”‚  Task      â”‚  â”‚
â”‚  â”‚  Similarityâ”‚  â”‚          â”‚  â”‚  Status    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Request Flow

### Query Processing Pipeline

```
User Query: "What are the key findings?"
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. CACHE CHECK                      â”‚ â—„â”€â”€ Cache hit? Return immediately
â”‚    Key: hash(query)                 â”‚     ~5ms
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Cache Miss
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. EMBEDDING GENERATION             â”‚ â—„â”€â”€ sentence-transformers
â”‚    Model: all-MiniLM-L6-v2          â”‚     ~20-50ms
â”‚    Dimensions: 384                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. VECTOR SEARCH (Go Service)       â”‚ â—„â”€â”€ HTTP POST /search
â”‚    Algorithm: HNSW                  â”‚     O(log n) complexity
â”‚    Returns: top-20 candidates       â”‚     ~10-50ms
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. CROSS-ENCODER RE-RANKING         â”‚ â—„â”€â”€ ms-marco-MiniLM-L-6-v2
â”‚    Input: (query, chunk) pairs      â”‚     Scores each pair
â”‚    Output: top-5 reordered          â”‚     ~50-200ms
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. CONTEXT ASSEMBLY                 â”‚ â—„â”€â”€ Token budget: 2048
â”‚    - Fit chunks to limit            â”‚     Add source citations
â”‚    - Build prompt template          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. LLM GENERATION                   â”‚ â—„â”€â”€ Ollama (llama2/mistral)
â”‚    Streaming response               â”‚     ~500ms-2s
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. CACHE & RETURN                   â”‚
â”‚    - Cache result (1h TTL)          â”‚
â”‚    - Return with sources            â”‚
â”‚    - Log metrics                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Tech Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **API Server** | Python + FastAPI | Orchestration, ML inference |
| **Vector Search** | Go + custom HNSW | High-performance similarity search |
| **Cache** | Redis | Embeddings, queries, task status |
| **Embeddings** | sentence-transformers | Text â†’ vectors |
| **Re-ranking** | Cross-encoder | Improve retrieval quality |
| **LLM** | Ollama / HuggingFace | Answer generation |
| **Frontend** | HTML/CSS/JS | User interface |
| **Container** | Docker Compose | Deployment |

---

## ğŸ“ Project Structure

```
DocuMind/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ docker-compose.yml           # Container orchestration
â”œâ”€â”€ .env.example                 # Environment template
â”‚
â”œâ”€â”€ go-vector-service/           # Go Vector Search Service
â”‚   â”œâ”€â”€ cmd/server/main.go       # Entry point
â”‚   â”œâ”€â”€ internal/
â”‚   â”‚   â”œâ”€â”€ api/                 # HTTP handlers & router
â”‚   â”‚   â”‚   â”œâ”€â”€ handlers.go
â”‚   â”‚   â”‚   â””â”€â”€ router.go
â”‚   â”‚   â””â”€â”€ index/               # Search algorithms
â”‚   â”‚       â”œâ”€â”€ similarity.go    # Cosine/L2 distance
â”‚   â”‚       â”œâ”€â”€ bruteforce.go    # O(n) baseline
â”‚   â”‚       â””â”€â”€ hnsw.go          # O(log n) ANN
â”‚   â”œâ”€â”€ pkg/types/vector.go      # Data types
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ go.mod
â”‚
â”œâ”€â”€ python-api/                  # Python FastAPI Backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI app
â”‚   â”‚   â”œâ”€â”€ config.py            # Settings
â”‚   â”‚   â”œâ”€â”€ api/routes/          # Endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ documents.py     # Upload/status
â”‚   â”‚   â”‚   â”œâ”€â”€ query.py         # Q&A
â”‚   â”‚   â”‚   â””â”€â”€ health.py        # Health/metrics
â”‚   â”‚   â”œâ”€â”€ core/                # ML components
â”‚   â”‚   â”‚   â”œâ”€â”€ chunking.py      # Text splitting
â”‚   â”‚   â”‚   â”œâ”€â”€ embeddings.py    # Vector generation
â”‚   â”‚   â”‚   â”œâ”€â”€ reranking.py     # Cross-encoder
â”‚   â”‚   â”‚   â”œâ”€â”€ context.py       # Prompt assembly
â”‚   â”‚   â”‚   â””â”€â”€ llm.py           # LLM clients
â”‚   â”‚   â”œâ”€â”€ services/            # External services
â”‚   â”‚   â”‚   â”œâ”€â”€ vector_client.py # Go service client
â”‚   â”‚   â”‚   â”œâ”€â”€ redis_client.py  # Cache operations
â”‚   â”‚   â”‚   â””â”€â”€ document_processor.py
â”‚   â”‚   â”œâ”€â”€ models/              # Pydantic schemas
â”‚   â”‚   â””â”€â”€ utils/metrics.py     # Performance logging
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”‚
â””â”€â”€ frontend/                    # Web UI
    â”œâ”€â”€ index.html
    â”œâ”€â”€ styles.css
    â”œâ”€â”€ app.js
    â”œâ”€â”€ nginx.conf
    â””â”€â”€ Dockerfile
```

---

## ğŸš€ Quick Start

### Prerequisites

- **Docker** & **Docker Compose**
- **Ollama** (optional, for LLM) - [Install Ollama](https://ollama.ai)
- **8GB+ RAM** recommended

### 1. Clone & Configure

```bash
git clone https://github.com/yourusername/documind.git
cd documind

# Copy environment template
cp .env.example .env
```

### 2. Start Ollama (Optional)

```bash
# Install Ollama, then:
ollama run llama2
# Or use mistral for better quality:
ollama run mistral
```

### 3. Launch with Docker Compose

```bash
# Build and start all services
docker-compose up --build

# Or run in background
docker-compose up -d --build
```

### 4. Access the Application

| Service | URL |
|---------|-----|
| **Frontend** | http://localhost:3000 |
| **API Docs** | http://localhost:8000/docs |
| **API Health** | http://localhost:8000/health |
| **Vector Service** | http://localhost:8001/health |

---

## ğŸ“Š API Reference

### Document Upload

```bash
# Upload a PDF
curl -X POST http://localhost:8000/api/documents/upload \
  -F "file=@document.pdf"

# Response
{
  "task_id": "abc123",
  "status": "processing"
}
```

### Check Processing Status

```bash
curl http://localhost:8000/api/documents/status/abc123

# Response
{
  "task_id": "abc123",
  "status": "completed",
  "progress": 1.0,
  "document_id": "xyz789",
  "num_chunks": 42
}
```

### Query Documents

```bash
curl -X POST http://localhost:8000/api/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What are the main conclusions?",
    "top_k": 5
  }'

# Response
{
  "answer": "Based on the documents, the main conclusions are...",
  "sources": [
    {
      "document_id": "xyz789",
      "chunk_text": "...",
      "relevance_score": 0.89
    }
  ],
  "latency": {
    "embedding_ms": 25.4,
    "search_ms": 8.2,
    "rerank_ms": 142.1,
    "llm_ms": 892.3,
    "total_ms": 1068.0
  }
}
```

---

## âš¡ Performance Targets

| Stage | p50 Target | p95 Target |
|-------|------------|------------|
| Query Embedding | < 20ms | < 50ms |
| Vector Search (10K vectors) | < 20ms | < 50ms |
| Cross-encoder Rerank | < 100ms | < 250ms |
| LLM Generation | < 1s | < 3s |
| **End-to-end (cached)** | < 50ms | < 100ms |
| **End-to-end (uncached)** | < 2s | < 5s |

---

## ğŸ§ª Running Tests

### Go Vector Service

```bash
cd go-vector-service

# Run tests
go test ./... -v

# Run benchmarks
go test -bench=. ./internal/index/ -benchmem
```

### Python API

```bash
cd python-api

# Create virtual environment
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt

# Run tests
pytest tests/ -v
```

---

## ğŸ›ï¸ Design Decisions & Tradeoffs

### Why Custom HNSW Instead of FAISS?

| Custom HNSW | FAISS |
|-------------|-------|
| âœ… Deep algorithmic understanding | âŒ Black box |
| âœ… "I built it" on resume | âŒ "I configured it" |
| âœ… Full control & customization | âŒ Limited flexibility |
| âš ï¸ Less optimized | âœ… Highly optimized |

**Decision**: Learning value and interview differentiation outweigh raw performance for a demo system.

### Why Go for Vector Search?

- **Goroutines**: Natural fit for parallel search
- **Memory efficiency**: Better control vs Python
- **Learning opportunity**: You're learning Go
- **Service boundary**: Clean separation enforces good architecture

### Why No LangChain?

| Without LangChain | With LangChain |
|-------------------|----------------|
| 50+ lines of code | 5 lines of code |
| Full visibility | Magic abstractions |
| "I understand RAG" | "I used a framework" |
| Easy to customize | Framework lock-in |

---

## ğŸš« What We Didn't Build (Intentionally)

| Feature | Reason to Skip |
|---------|----------------|
| Multi-node sharding | Overkill for demo, adds complexity |
| GPU support | CPU-only keeps it accessible |
| Authentication | Not the focus of the demo |
| Kubernetes | Docker Compose is sufficient |
| Fine-tuned models | Pre-trained works well enough |
| Real-time sync | Batch processing is fine |

---

## ğŸ“ˆ Resume Bullet Points

After completing this project, add these to your resume:

> **DocuMind | Production-Grade Document Intelligence System**
> - Designed and implemented a **RAG system** achieving <50ms p50 latency for cached queries using custom vector search, Redis caching, and async processing
> - Built a **custom HNSW graph-based ANN search** in Go from scratch, demonstrating O(log n) vs O(n) brute-force complexity
> - Implemented **microservices architecture** with Python (FastAPI) and Go, featuring goroutine-based concurrency, cross-encoder re-ranking, and token-aware context assembly
> - Developed **complete RAG pipeline without high-level frameworks** (no LangChain), including manual chunking, embedding generation, and prompt engineering

### Skills Demonstrated

âœ… Systems Design & Distributed Architecture  
âœ… Algorithm Implementation (HNSW, Similarity Search)  
âœ… Performance Engineering (Caching, Concurrency)  
âœ… ML Engineering (Embeddings, Cross-encoders, RAG)  
âœ… Multi-language Development (Python + Go)  
âœ… Observability (Latency Tracking, p50/p95 Metrics)

---

## ğŸ”§ Development

### Local Development (without Docker)

```bash
# Terminal 1: Redis
docker run -p 6379:6379 redis:7-alpine

# Terminal 2: Go Vector Service
cd go-vector-service
go run ./cmd/server

# Terminal 3: Python API
cd python-api
pip install -r requirements.txt
uvicorn app.main:app --reload

# Terminal 4: Frontend (optional)
cd frontend
python -m http.server 3000
```

### Environment Variables

See `.env.example` for all configuration options.

---

## ğŸ“„ License

MIT License - feel free to use this for learning, portfolios, and interviews.

---

## ğŸ™ Acknowledgments

- [Sentence-Transformers](https://www.sbert.net/) for embedding models
- [HNSW Paper](https://arxiv.org/abs/1603.09320) by Malkov & Yashunin
- [Ollama](https://ollama.ai) for local LLM inference

---

**Built with â¤ï¸ for learning and demonstrating systems engineering skills.**
